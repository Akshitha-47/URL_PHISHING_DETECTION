{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import metrics \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data into dataframe\n",
    "\n",
    "data = pd.read_csv(\"phishing.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of dataframe\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing the features of the dataset\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information about the dataset\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nunique value in columns\n",
    "\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping index column\n",
    "\n",
    "data = data.drop(['Index'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#description of dataset\n",
    "\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation heatmap\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(data.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phishing Count in pie chart\n",
    "\n",
    "data['class'].value_counts().plot(kind='pie',autopct='%1.2f%%')\n",
    "plt.title(\"Phishing Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into dependant and independant fetature\n",
    "\n",
    "X = data.drop([\"class\"],axis =1)\n",
    "y = data[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test sets: 80-20 split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating holders to store the model performance results\n",
    "ML_Model = []\n",
    "accuracy = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "\n",
    "#function to call for storing the results\n",
    "def storeResults(model, a,b,c,d):\n",
    "  ML_Model.append(model)\n",
    "  accuracy.append(round(a, 3))\n",
    "  f1_score.append(round(b, 3))\n",
    "  recall.append(round(c, 3))\n",
    "  precision.append(round(d, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate the model\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# fit the model \n",
    "forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the target value from the model for the samples\n",
    "y_train_forest = forest.predict(X_train)\n",
    "y_test_forest = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the accuracy, f1_score, Recall, precision of the model performance\n",
    "\n",
    "acc_train_forest = metrics.accuracy_score(y_train,y_train_forest)\n",
    "acc_test_forest = metrics.accuracy_score(y_test,y_test_forest)\n",
    "print(\"Random Forest : Accuracy on training Data: {:.3f}\".format(acc_train_forest))\n",
    "print(\"Random Forest : Accuracy on test Data: {:.3f}\".format(acc_test_forest))\n",
    "print()\n",
    "\n",
    "f1_score_train_forest = metrics.f1_score(y_train,y_train_forest)\n",
    "f1_score_test_forest = metrics.f1_score(y_test,y_test_forest)\n",
    "print(\"Random Forest : f1_score on training Data: {:.3f}\".format(f1_score_train_forest))\n",
    "print(\"Random Forest : f1_score on test Data: {:.3f}\".format(f1_score_test_forest))\n",
    "print()\n",
    "\n",
    "recall_score_train_forest = metrics.recall_score(y_train,y_train_forest)\n",
    "recall_score_test_forest = metrics.recall_score(y_test,y_test_forest)\n",
    "print(\"Random Forest : Recall on training Data: {:.3f}\".format(recall_score_train_forest))\n",
    "print(\"Random Forest : Recall on test Data: {:.3f}\".format(recall_score_test_forest))\n",
    "print()\n",
    "\n",
    "precision_score_train_forest = metrics.precision_score(y_train,y_train_forest)\n",
    "precision_score_test_forest = metrics.precision_score(y_test,y_test_forest)\n",
    "print(\"Random Forest : precision on training Data: {:.3f}\".format(precision_score_train_forest))\n",
    "print(\"Random Forest : precision on test Data: {:.3f}\".format(precision_score_test_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the classification report of the model\n",
    "\n",
    "print(metrics.classification_report(y_test, y_test_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try max_depth from 1 to 20\n",
    "depth = range(1,20)\n",
    "for n in depth:\n",
    "    forest_test = RandomForestClassifier(n_estimators=100, max_depth=n, random_state=42)\n",
    "\n",
    "    forest_test.fit(X_train, y_train)\n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(forest_test.score(X_train, y_train))\n",
    "    # record generalization accuracy\n",
    "    test_accuracy.append(forest_test.score(X_test, y_test))\n",
    "    \n",
    "\n",
    "#plotting the training & testing accuracy for n_estimators from 1 to 20\n",
    "plt.figure(figsize=None)\n",
    "plt.plot(depth, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(depth, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")  \n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.title(\"Training & Testing Accuracy for Random Forest\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "# instantiate the model\n",
    "log = LogisticRegression()\n",
    "\n",
    "# fit the model \n",
    "log.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the target value from the model for the samples\n",
    "\n",
    "y_train_log = log.predict(X_train)\n",
    "y_test_log = log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the accuracy, f1_score, Recall, precision of the model performance\n",
    "\n",
    "acc_train_log = metrics.accuracy_score(y_train,y_train_log)\n",
    "acc_test_log = metrics.accuracy_score(y_test,y_test_log)\n",
    "print(\"Logistic Regression : Accuracy on training Data: {:.3f}\".format(acc_train_log))\n",
    "print(\"Logistic Regression : Accuracy on test Data: {:.3f}\".format(acc_test_log))\n",
    "print()\n",
    "\n",
    "f1_score_train_log = metrics.f1_score(y_train,y_train_log)\n",
    "f1_score_test_log = metrics.f1_score(y_test,y_test_log)\n",
    "print(\"Logistic Regression : f1_score on training Data: {:.3f}\".format(f1_score_train_log))\n",
    "print(\"Logistic Regression : f1_score on test Data: {:.3f}\".format(f1_score_test_log))\n",
    "print()\n",
    "\n",
    "recall_score_train_log = metrics.recall_score(y_train,y_train_log)\n",
    "recall_score_test_log = metrics.recall_score(y_test,y_test_log)\n",
    "print(\"Logistic Regression : Recall on training Data: {:.3f}\".format(recall_score_train_log))\n",
    "print(\"Logistic Regression : Recall on test Data: {:.3f}\".format(recall_score_test_log))\n",
    "print()\n",
    "\n",
    "precision_score_train_log = metrics.precision_score(y_train,y_train_log)\n",
    "precision_score_test_log = metrics.precision_score(y_test,y_test_log)\n",
    "print(\"Logistic Regression : precision on training Data: {:.3f}\".format(precision_score_train_log))\n",
    "print(\"Logistic Regression : precision on test Data: {:.3f}\".format(precision_score_test_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the classification report of the model\n",
    "\n",
    "print(metrics.classification_report(y_test, y_test_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lists to store training and testing accuracy\n",
    "training_accuracy_logreg = []\n",
    "test_accuracy_logreg = []\n",
    "\n",
    "# Try different values of the regularization parameter C\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for C in C_values:\n",
    "\n",
    "    # Record training set accuracy\n",
    "    training_accuracy_logreg.append(log.score(X_train, y_train))\n",
    "\n",
    "    # Record generalization accuracy\n",
    "    test_accuracy_logreg.append(log.score(X_test, y_test))\n",
    "\n",
    "# Plotting the training & testing accuracy for different values of C\n",
    "plt.plot(C_values, training_accuracy_logreg, label=\"Training accuracy\")\n",
    "plt.plot(C_values, test_accuracy_logreg, label=\"Testing accuracy\")\n",
    "plt.xscale('log')  # Using a logarithmic scale for better visualization\n",
    "plt.xlabel(\"Regularization Parameter (C)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training & Testing Accuracy for Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "\n",
    "storeResults('Logistic Regression',acc_test_log,f1_score_test_log,\n",
    "             recall_score_train_log,precision_score_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# instantiate the model \n",
    "tree = DecisionTreeClassifier(max_depth=30)\n",
    "\n",
    "# fit the model \n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the target value from the model for the samples\n",
    "\n",
    "y_train_tree = tree.predict(X_train)\n",
    "y_test_tree = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the accuracy, f1_score, Recall, precision of the model performance\n",
    "\n",
    "acc_train_tree = metrics.accuracy_score(y_train,y_train_tree)\n",
    "acc_test_tree = metrics.accuracy_score(y_test,y_test_tree)\n",
    "print(\"Decision Tree : Accuracy on training Data: {:.3f}\".format(acc_train_tree))\n",
    "print(\"Decision Tree : Accuracy on test Data: {:.3f}\".format(acc_test_tree))\n",
    "print()\n",
    "\n",
    "f1_score_train_tree = metrics.f1_score(y_train,y_train_tree)\n",
    "f1_score_test_tree = metrics.f1_score(y_test,y_test_tree)\n",
    "print(\"Decision Tree : f1_score on training Data: {:.3f}\".format(f1_score_train_tree))\n",
    "print(\"Decision Tree : f1_score on test Data: {:.3f}\".format(f1_score_test_tree))\n",
    "print()\n",
    "\n",
    "recall_score_train_tree = metrics.recall_score(y_train,y_train_tree)\n",
    "recall_score_test_tree = metrics.recall_score(y_test,y_test_tree)\n",
    "print(\"Decision Tree : Recall on training Data: {:.3f}\".format(recall_score_train_tree))\n",
    "print(\"Decision Tree : Recall on test Data: {:.3f}\".format(recall_score_test_tree))\n",
    "print()\n",
    "\n",
    "precision_score_train_tree = metrics.precision_score(y_train,y_train_tree)\n",
    "precision_score_test_tree = metrics.precision_score(y_test,y_test_tree)\n",
    "print(\"Decision Tree : precision on training Data: {:.3f}\".format(precision_score_train_tree))\n",
    "print(\"Decision Tree : precision on test Data: {:.3f}\".format(precision_score_test_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the classification report of the model\n",
    "\n",
    "print(metrics.classification_report(y_test, y_test_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try max_depth from 1 to 30\n",
    "depth = range(1,30)\n",
    "for n in depth:\n",
    "    tree_test = DecisionTreeClassifier(max_depth=n)\n",
    "\n",
    "    tree_test.fit(X_train, y_train)\n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(tree_test.score(X_train, y_train))\n",
    "    # record generalization accuracy\n",
    "    test_accuracy.append(tree_test.score(X_test, y_test))\n",
    "    \n",
    "\n",
    "#plotting the training & testing accuracy for max_depth from 1 to 30\n",
    "plt.plot(depth, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(depth, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")  \n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "\n",
    "storeResults('Decision Tree',acc_test_tree,f1_score_test_tree,\n",
    "             recall_score_train_tree,precision_score_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes Classifier model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# instantiate the model\n",
    "nb = GaussianNB()\n",
    "\n",
    "# fit the model\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the target value from the model for the samples\n",
    "y_train_nb = nb.predict(X_train)\n",
    "y_test_nb = nb.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the accuracy, f1_score, Recall, precision of the model performance\n",
    "acc_train_nb = metrics.accuracy_score(y_train, y_train_nb)\n",
    "acc_test_nb = metrics.accuracy_score(y_test, y_test_nb)\n",
    "print(\"Gaussian Naive Bayes: Accuracy on training Data: {:.3f}\".format(acc_train_nb))\n",
    "print(\"Gaussian Naive Bayes: Accuracy on test Data: {:.3f}\".format(acc_test_nb))\n",
    "print()\n",
    "\n",
    "f1_score_train_nb = metrics.f1_score(y_train, y_train_nb)\n",
    "f1_score_test_nb = metrics.f1_score(y_test, y_test_nb)\n",
    "print(\"Gaussian Naive Bayes: f1_score on training Data: {:.3f}\".format(f1_score_train_nb))\n",
    "print(\"Gaussian Naive Bayes: f1_score on test Data: {:.3f}\".format(f1_score_test_nb))\n",
    "print()\n",
    "\n",
    "recall_score_train_nb = metrics.recall_score(y_train, y_train_nb)\n",
    "recall_score_test_nb = metrics.recall_score(y_test, y_test_nb)\n",
    "print(\"Gaussian Naive Bayes: Recall on training Data: {:.3f}\".format(recall_score_train_nb))\n",
    "print(\"Gaussian Naive Bayes: Recall on test Data: {:.3f}\".format(recall_score_test_nb))\n",
    "print()\n",
    "\n",
    "precision_score_train_nb = metrics.precision_score(y_train, y_train_nb)\n",
    "precision_score_test_nb = metrics.precision_score(y_test, y_test_nb)\n",
    "print(\"Gaussian Naive Bayes: precision on training Data: {:.3f}\".format(precision_score_train_nb))\n",
    "print(\"Gaussian Naive Bayes: precision on test Data: {:.3f}\".format(precision_score_test_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# computing the classification report of the model\n",
    "print(metrics.classification_report(y_test, y_test_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_accuracy_nb = []\n",
    "test_accuracy_nb = []\n",
    "# try different priors\n",
    "priors = [None, [0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.5, 0.5]]\n",
    "for prior in priors:\n",
    "    nb_test = GaussianNB(priors=prior)\n",
    "\n",
    "    nb_test.fit(X_train, y_train)\n",
    "    # record training set accuracy\n",
    "    training_accuracy_nb.append(nb_test.score(X_train, y_train))\n",
    "    # record generalization accuracy\n",
    "    test_accuracy_nb.append(nb_test.score(X_test, y_test))\n",
    "\n",
    "# plotting the training & testing accuracy for different priors\n",
    "plt.plot(range(len(priors)), training_accuracy_nb, label=\"training accuracy\")\n",
    "plt.plot(range(len(priors)), test_accuracy_nb, label=\"test accuracy\")\n",
    "plt.xticks(range(len(priors)), [str(prior) for prior in priors])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Priors\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# storing the results. The below mentioned order of parameter passing is important.\n",
    "storeResults('Gaussian Naive Bayes', acc_test_nb, f1_score_test_nb,\n",
    "             recall_score_train_nb, precision_score_train_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer Perceptron Classifier Model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# instantiate the model\n",
    "mlp = MLPClassifier()\n",
    "#mlp = GridSearchCV(mlpc, parameter_space)\n",
    "\n",
    "# fit the model \n",
    "mlp.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#predicting the target value from the model for the samples\n",
    "y_train_mlp = mlp.predict(X_train)\n",
    "y_test_mlp = mlp.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#computing the accuracy, f1_score, Recall, precision of the model performance\n",
    "\n",
    "acc_train_mlp  = metrics.accuracy_score(y_train,y_train_mlp)\n",
    "acc_test_mlp = metrics.accuracy_score(y_test,y_test_mlp)\n",
    "print(\"Multi-layer Perceptron : Accuracy on training Data: {:.3f}\".format(acc_train_mlp))\n",
    "print(\"Multi-layer Perceptron : Accuracy on test Data: {:.3f}\".format(acc_test_mlp))\n",
    "print()\n",
    "\n",
    "f1_score_train_mlp = metrics.f1_score(y_train,y_train_mlp)\n",
    "f1_score_test_mlp = metrics.f1_score(y_test,y_test_mlp)\n",
    "print(\"Multi-layer Perceptron : f1_score on training Data: {:.3f}\".format(f1_score_train_mlp))\n",
    "print(\"Multi-layer Perceptron : f1_score on test Data: {:.3f}\".format(f1_score_train_mlp))\n",
    "print()\n",
    "\n",
    "recall_score_train_mlp = metrics.recall_score(y_train,y_train_mlp)\n",
    "recall_score_test_mlp = metrics.recall_score(y_test,y_test_mlp)\n",
    "print(\"Multi-layer Perceptron : Recall on training Data: {:.3f}\".format(recall_score_train_mlp))\n",
    "print(\"Multi-layer Perceptron : Recall on test Data: {:.3f}\".format(recall_score_test_mlp))\n",
    "print()\n",
    "\n",
    "precision_score_train_mlp = metrics.precision_score(y_train,y_train_mlp)\n",
    "precision_score_test_mlp = metrics.precision_score(y_test,y_test_mlp)\n",
    "print(\"Multi-layer Perceptron : precision on training Data: {:.3f}\".format(precision_score_train_mlp))\n",
    "print(\"Multi-layer Perceptron : precision on test Data: {:.3f}\".format(precision_score_test_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#computing the classification report of the model\n",
    "\n",
    "print(metrics.classification_report(y_test, y_test_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "depth=range(1,20)\n",
    "for n in depth:\n",
    "    mlp_test = MLPClassifier(learning_rate_init=n*0.1, max_iter=200, random_state=42)\n",
    "\n",
    "    mlp_test.fit(X_train, y_train)\n",
    "    # Record training set accuracy\n",
    "    training_accuracy.append(mlp_test.score(X_train, y_train))\n",
    "    # Record generalization accuracy\n",
    "    test_accuracy.append(mlp_test.score(X_test, y_test))\n",
    "\n",
    "# Plotting the training & testing accuracy for different learning rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depth, training_accuracy, label=\"Training accuracy\")\n",
    "plt.plot(depth, test_accuracy, label=\"Testing accuracy\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training & Testing Accuracy for MLP\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "\n",
    "storeResults('Multi-layer Perceptron',acc_test_mlp,f1_score_test_mlp,\n",
    "             recall_score_train_mlp,precision_score_train_mlp)\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "result = pd.DataFrame({ 'ML Model' : ML_Model,\n",
    "                        'Accuracy' : accuracy,\n",
    "                        'f1_score' : f1_score,\n",
    "                        'Recall'   : recall,\n",
    "                        'Precision': precision,\n",
    "                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting the datafram on accuracy\n",
    "sorted_result=result.sort_values(by=['Accuracy', 'f1_score'],ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_Model = ['Random Forest', 'LogisticRegression','DecisionTreeClassifier ','Gaussian Naive Bayes', 'Multi-layer Perceptron']\n",
    "accuracy = [acc_test_forest, acc_test_log, acc_test_tree, acc_test_nb, acc_test_mlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'ML Model': ML_Model,\n",
    "    'Accuracy': accuracy\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sorting the DataFrame based on Accuracy\n",
    "sorted_results = results.sort_values(by=['Accuracy'], ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(sorted_result['ML Model'], sorted_result['Accuracy'], color=['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon'])\n",
    "plt.title('Testing Accuracy Comparison of ML Models')\n",
    "plt.xlabel('ML Model')\n",
    "plt.ylabel('Testing Accuracy')\n",
    "plt.ylim(0, 1)  # Set the y-axis limit between 0 and 1 for accuracy values\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Displaying precise values on top of each bar\n",
    "for bar, value in zip(bars, sorted_result['Accuracy']):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2 - 0.1, bar.get_height() + 0.01, f'{value:.3f}', ha='center', color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispalying total result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispalying total result\n",
    "sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer Perceptron Classifier Model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# instantiate the model\n",
    "# Assuming you want a single hidden layer with 4 neurons\n",
    "Bestmodel = MLPClassifier(hidden_layer_sizes=(4,), learning_rate_init=0.7)\n",
    "\n",
    "# fit the model \n",
    "Bestmodel.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the pickel model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "directory = 'pickle1'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Dump information to that file\n",
    "pickle.dump(Bestmodel, open(os.path.join(directory, 'model.pkl'), 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
